{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner.png](banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">A. System Overview</h2>\n",
    "\n",
    "This machine learning system is designed to showcase an end-to-end implementation covering the entire lifecycle of a machine learning model. It integrates various MLOps tools and technologies discussed during the DASCI 270 sessions, that includes facilitating data ingestion, preprocessing, training, validation, deployment, and monitoring of the model. The system is also structured to handle drift detection to ensure the model remains effective and accurate over time. This document will guide you through interacting with the deployed **Equipment Failure Prediction** system, including making predictions, retrieving model information, and checking data drift, as well as provide detailed documentation of each component and their functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">Components</h2>\n",
    "\n",
    "1. **Machine Learning Model** - This system utilizes XGBoost, a highly efficient and scalable machine learning algorithm for classification tasks. It serves as the core predictive model in our system, specifically designed to predict equipment failure.\n",
    "\n",
    "2. **Data Pipeline (Dagster)** - Orchestrates the workflow for data ingestion, preprocessing, and preparation. Dagster manages the sequence of these operations to ensure data flows correctly from one process to another, maintaining a clear and manageable execution order.\n",
    "\n",
    "3. **Experiment Tracking (MLflow)** - Provides a framework to track experiments, including model training runs, parameters, metrics, and artifacts, enabling easier debugging and optimization. It stores models, performance metrics, and custom objects like drift reports, making them easy to access and compare across different runs.\n",
    "\n",
    "4. **Model Serving (FastAPI)** - Deploys the trained model through a REST API using FastAPI, facilitating easy access to the modelâ€™s predictive capabilities. The API handles requests for predictions and provides model metadata, ensuring input validation and structured outputs.\n",
    "\n",
    "5. **Containerization (Docker)** - Containerizes the services making up the system to ensure consistency and reproducibility across environments. Each isolated environment (\"container\") contains all necessary dependencies for each service, which can be easily deployed on any system supporting Docker.\n",
    "\n",
    "6. **Drift Detection (Evidently AI)** - Integrates Evidently AI to monitor the model for any signs of data or concept drift. This component is crucial for maintaining the model's accuracy, providing insights into how the data characteristics and relationships are changing over time.\n",
    "\n",
    "7. **Data Validation (Pydantic)** - Ensures that the data received by the API matches the expected format and type. This prevents errors during model prediction and ensures reliable model performance.\n",
    "\n",
    "8. **Testing (Pytest and Github Actions)** - Uses Pytest to develop and run comprehensive tests that validate the correctness of the data processing and feature engineering components. GitHub Actions automates these tests, ensuring that all code integrations meet quality standards and function as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">Dataset</h2>\n",
    "\n",
    "This project utilized the AI4I 2020 Predictive Maintenance dataset, a synthetic dataset sourced from UCI Machine Learning Repository: https://archive.ics.uci.edu/dataset/601/ai4i+2020+predictive+maintenance+dataset. It contains 10,000 data points with the following columns/variables:\n",
    "\n",
    "| Variables          | Type        | Description                                                                                         |\n",
    "|--------------------|-------------|-----------------------------------------------------------------------------------------------------|\n",
    "| UID                | Integer     | Unique identifier ranging from 1 to 10000                                                           |\n",
    "| Product ID         | Categorical | Consists of a letter (L, M, H) for low, medium, and high product quality variants with a serial number |\n",
    "| Type               | Categorical | Not specified                                                                                       |\n",
    "| Air temperature    | Continuous  | Generated using a random walk process, normalized around 300 K with a standard deviation of 2 K      |\n",
    "| Process temperature| Continuous  | Generated using a random walk process, normalized to a standard deviation of 1 K, plus air temperature + 10 K |\n",
    "| Rotational speed   | Integer     | Calculated based on a power of 2860 W with normally distributed noise                               |\n",
    "| Torque             | Continuous  | Normally distributed around 40 Nm with a standard deviation of 10 Nm, no negative values             |\n",
    "| Tool wear          | Integer     | Tool wear added by quality variants H/M/L are 5/3/2 minutes respectively                             |\n",
    "| Machine failure    | Integer     | Indicates if the machine has failed (1) or not (0) in this particular datapoint                      |\n",
    "| TWF                | Integer     | Target feature indicating if a specific type of failure occurred or not (1/0)                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">B. Setup Instructions</h2>\n",
    "\n",
    "This section provides detailed instructions on how to set up and run the system using Docker Compose. The instructions assume you have Docker and Docker Compose installed on your machine. If not, please install them from the [Docker official website](https://www.docker.com/) before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">1. Clone the Repository</h2>\n",
    "\n",
    "Begin by cloning the project repository from GitHub. This will retrieve all the necessary code and configuration files needed to set up and run the project locally. Execute the following commands in your terminal:\n",
    "\n",
    "```bash\n",
    "# Clone the repository\n",
    "git clone https://github.com/raymundojavajr/ml-fp.git\n",
    "\n",
    "# Change directory to the cloned repository\n",
    "cd ml-fp\n",
    "```\n",
    "\n",
    "These commands clone the repository into a directory named `ml-fp` on your local machine and then change your current directory to `ml-fp`, placing you in the project's root directory where subsequent setup steps are performed.\n",
    "\n",
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">2. Configure Environment Variables</h2>\n",
    "\n",
    "To manage environment variables, create a `.env` file in the root directory of the project. This file will store all necessary environment settings. Ensure this file is not tracked by version control by adding it to your `.gitignore` file.\n",
    "\n",
    "For local development, you'll also need to set up a virtual environment to isolate your project's dependencies from the global Python environment. Here's how to set up and activate a virtual environment using Python's built-in `venv` module:\n",
    "\n",
    "```bash\n",
    "# Create a virtual environment\n",
    "python -m venv venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "# On Windows, use:\n",
    "venv\\Scripts\\activate\n",
    "\n",
    "# On Unix or MacOS, use:\n",
    "source venv/bin/activate\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "If you are using uv as your package manager, ensure you have it installed and then synchronize your environment:\n",
    "\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "\n",
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">3. Install Dependencies/Packages</h2>\n",
    "\n",
    "Ensure you have the necessary Python packages installed:\n",
    "\n",
    "```bash\n",
    "pip install requests pandas mlflow evidently fastapi uvicorn dagster dagster-docker dagster-postgres\n",
    "```\n",
    "\n",
    "#### **List of Libraries used in the pipeline:**\n",
    "\n",
    "The following libraries are integral to the pipeline, supporting various aspects of machine learning model development, deployment, and maintenance:\n",
    "\n",
    "**Core Libraries**\n",
    "\n",
    "- mlflow - Tracks experiments, including model training runs, parameters, and metrics.\n",
    "- pandas - Data manipulation and analysis.\n",
    "- matplotlib - Used for creating static, interactive, and animated visualizations in Python.\n",
    "- joblib - Efficient saving of models and other large data structures.\n",
    "- requests - Handles HTTP requests to external services or APIs.\n",
    "\n",
    "**Machine Learning Models and Tools**\n",
    "\n",
    "- xgboost - Implements machine learning algorithms under the Gradient Boosting framework.\n",
    "- scikit-learn - Tools for data mining and data analysis, including model training and evaluation functions.\n",
    "- shap - Explains the output of machine learning models.\n",
    "- evidently - Monitors model performance and detects data drift.\n",
    "\n",
    "**Dagster Integration**\n",
    "\n",
    "- dagster, dagit, dagster-postgres, dagster-pandas, dagster-mlflow - Provides orchestration of data workflows, asset management, and integration with other ML tools.\n",
    "\n",
    "**API Development and Deployment**\n",
    "\n",
    "- fastapi - Modern, fast (high-performance) web framework for building APIs.\n",
    "- uvicorn - ASGI server for FastAPI, which allows async programming.\n",
    "- pydantic - Data validation by defining data types and structures for FastAPI endpoints.\n",
    "- python-dotenv - Reads key-value pairs from a .env file and sets them as environment variables.\n",
    "\n",
    "**Utilities and Miscellaneous**\n",
    "\n",
    "- matplotlib - For plotting and visualizing data in various forms.\n",
    "- pydantic - For data validation and settings management via data models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">C. Orchestrating ML Workflows with Dagster</h2>\n",
    "\n",
    "This section explains the execution of the **Machine Failure Prediction** pipeline, which is fully managed within Dagster. This includes setting up and running the pipeline components via the Dagster UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline Overview**\n",
    "\n",
    "The pipeline is designed to handle various stages from data processing to model deployment, ensuring thorough analysis and prediction of machine failures.\n",
    "\n",
    "#### **Dagster Assets in the Pipeline**\n",
    "\n",
    "- **Data Ingestion (Upstream Assets):**\n",
    "  - `download_machine_data` retrieves raw sensor and operation data from sources.\n",
    "  - `raw_machine_data` stores the fetched data for subsequent processing.\n",
    "- **Data Preparation (Downstream from Raw Data):**\n",
    "  - `cleaned_machine_data` performs data cleaning and preprocessing to prepare for analysis.\n",
    "- **Model Development (Downstream from Prepared Data):**\n",
    "  - `train_test_split_data` segments the data into training and testing subsets.\n",
    "  - `feature_preprocessor` applies necessary transformations for the predictive model.\n",
    "  - `trained_model` encompasses the training of the failure prediction model.\n",
    "- **Operational Deployment (Downstream from Model Development):**\n",
    "  - `model_evaluation` assesses the predictive accuracy and robustness of the model.\n",
    "  - `saved_model` handles the storage of the validated model.\n",
    "- **Continuous Improvement (Downstream from Deployment):**\n",
    "  - `drift_detection_data` simulates potential drift in machine operation data.\n",
    "  - `cleaned_drift_data` prepares this data for drift analysis.\n",
    "  - `preprocessed_drift_data` further processes the data to fit analysis models.\n",
    "  - `drift_reports` creates detailed reports on any detected drift, helping in proactive maintenance.\n",
    "\n",
    "#### **Executing the Pipeline**\n",
    "\n",
    "To operationalize all assets in the **Machine Failure Prediction** pipeline, navigate to the **Dagster UI** at (`http://localhost:3000`) and follow these steps:\n",
    "\n",
    "- **Start with Upstream Assets:** Initiate the pipeline by materializing assets such as `download_machine_data` and `raw_machine_data`.\n",
    "- **Advance through the Pipeline:** Continue activating downstream assets in sequence to ensure a logical flow of data processing and model training.\n",
    "- **Finalize with Evaluation and Monitoring:** Complete the execution by materializing assets related to model evaluation and drift detection, which are crucial for maintaining the accuracy and relevance of the prediction model.\n",
    "\n",
    "This structured approach ensures that each component of the pipeline functions efficiently and that dependencies are correctly managed from data collection through to drift detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">D. Deploying the Full ML Pipeline Stack with Docker Compose</h2>\n",
    "\n",
    "This project leverages Docker Compose for containerization, ensuring cohesive and efficient functioning of all components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Containerized Services in `docker-compose.yaml`**\n",
    "\n",
    "Below are the services configured in the `docker-compose.yaml` file, outlining their roles and the ports they operate on:\n",
    "\n",
    "| **Service** | **Purpose** | **Port** |\n",
    "|-------------|-------------|---------|\n",
    "| **PostgreSQL (`postgres_service`)** | Database for storing pipeline data | Internal |\n",
    "| **MinIO (`minio_service`)** | S3-compatible storage for artifacts | `9000`, `9001` |\n",
    "| **MLflow Tracking Server (`mlflow_service`)** | Manages experiment tracking and artifact logging | `5000` |\n",
    "| **FastAPI Model Server (`fastapi_service`)** | Hosts the machine failure prediction model | `8000` |\n",
    "| **Dagster Server (`dagster_service`)** | Orchestrates and monitors the pipeline | `3000` |\n",
    "\n",
    "#### Step 1: Launch All Services\n",
    "Execute the following command to build and initiate all services:\n",
    "\n",
    "```bash\n",
    "docker-compose up --build -d\n",
    "```\n",
    "\n",
    "This command:\n",
    "- Constructs any necessary Docker images.\n",
    "- Launches all services in detached mode (`-d`).\n",
    "\n",
    "#### Step 2: Verify Service Containers\n",
    "To confirm that all Docker containers are active, run:\n",
    "\n",
    "```bash\n",
    "docker ps\n",
    "```\n",
    "\n",
    "#### Step 3: Service Access Points\n",
    "Here are the URLs to access each service's user interface:\n",
    "\n",
    "| **Service**               | **URL**                                 |\n",
    "|---------------------------|-----------------------------------------|\n",
    "| **Dagster UI**            | [http://localhost:3000](http://localhost:3000) |\n",
    "| **MLflow Tracking UI**    | [http://localhost:5000](http://localhost:5000) |\n",
    "| **FastAPI Model Server**  | [http://localhost:8000](http://localhost:8000) |\n",
    "| **MinIO Console**         | [http://localhost:9001](http://localhost:9001) |\n",
    "\n",
    "#### Step 4: Configure MinIO for MLflow\n",
    "Prior to using Dagster for asset materialization, configure a storage bucket in MinIO:\n",
    "\n",
    "1. Visit [http://localhost:9001](http://localhost:9001)\n",
    "2. Log in using:\n",
    "   - **Username:** `minio_user`\n",
    "   - **Password:** `minio_password`\n",
    "3. Establish a new bucket named `mlflow`.\n",
    "4. Amend your `.env` file with the MinIO Access Keys obtained:\n",
    "\n",
    "   ```ini\n",
    "   # MinIO Access Keys\n",
    "   MINIO_ACCESS_KEY=your_generated_access_key\n",
    "   MINIO_SECRET_ACCESS_KEY=your_generated_secret_key\n",
    "   ```\n",
    "\n",
    "#### Step 5: Materialize Assets in Dagster\n",
    "With all services operational, proceed to materialize assets:\n",
    "\n",
    "- Navigate to the [Dagster UI](http://localhost:3000).\n",
    "- Select \"Assets\" â†’ \"Materialize Selected\" to execute the pipeline.\n",
    "\n",
    "### ðŸ“Œ Service Management Commands\n",
    "\n",
    "| **Action**                | **Command**                          |\n",
    "|---------------------------|--------------------------------------|\n",
    "| **Restart all services**  | `docker-compose down && docker-compose up -d` |\n",
    "| **Stop all services**     | `docker-compose down`                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">E. Happy Path</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">Prediction Request</h2>\n",
    "\n",
    "In this section, we demonstrate how to use the FastAPI model server for predictions. We'll send a set of equipment parameters and status data to receive predictions on potential machine failures.\n",
    "\n",
    "- **Input Data:** This includes a series of variables such as equipment type, air and process temperatures, rotational speed, torque, tool wear, and previous instances of machine failure.\n",
    "- **API Endpoint:** This is the URL where the FastAPI server accepts prediction requests.\n",
    "- **Response Handling:** Upon receiving the prediction request, the server provides a forecast indicating whether a machine is likely to fail. This prediction is displayed in the output.\n",
    "\n",
    "Execute the following cell to initiate a prediction request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# FastAPI server URL\n",
    "FASTAPI_URL = \"http://localhost:8000/predict\"\n",
    "\n",
    "# Define the input data\n",
    "# In order of the features: UDI, Air temperature (K), Process temperature (K), Rotational speed (rpm), Torque (Nm), Tool wear (min), Type encoded, Product ID encoded, Failure Type encoded\n",
    "# Ensure to adjust the data based on actual possible inputs and features expected by the model\n",
    "payload = {\n",
    "    \"features\": [\n",
    "        2,          # UDI\n",
    "        310.5,      # Air temperature in Kelvin\n",
    "        320.1,      # Process temperature in Kelvin\n",
    "        2000,       # Rotational speed in rpm\n",
    "        50.5,       # Torque in Nm\n",
    "        10,         # Tool wear in minutes\n",
    "        1,          # Type encoded (categorical, numeric encoding)\n",
    "        8005,       # Product ID encoded (categorical, numeric encoding)\n",
    "        0           # Failure Type encoded (target variable, if used for retraining or similar scenarios)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Send request to FastAPI\n",
    "response = requests.post(FASTAPI_URL, json=payload)\n",
    "\n",
    "# Handle response\n",
    "if response.status_code == 200:\n",
    "    print(\"Prediction:\", response.json())  # Print the predicted output, likely a machine failure prediction or similar\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)  # Handle errors like 404 or 500 from server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response Interpretation\n",
    "\n",
    "# Insert explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">Model Information Retrieval</h2>\n",
    "\n",
    "To effectively interact with our FastAPI model server, it's essential to retrieve the model's configuration. This process involves accessing detailed metadata from the /model endpoint, which helps users understand the underlying model's structure and operational parameters.\n",
    "\n",
    "**Retrieve Model Information**\n",
    "\n",
    "The model server offers detailed metadata which includes:\n",
    "\n",
    "* **Input schema** detailing the required format for prediction inputs.\n",
    "* **Training Hyperparameters** used during the training process.\n",
    "* **Important features** that significantly impact the prediction outcomes.\n",
    "\n",
    "Execute the following cell to obtain detailed information about the model hosted on FastAPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# FastAPI server URL for the /model endpoint\n",
    "FASTAPI_URL = \"http://localhost:8000/model\"\n",
    "\n",
    "# Send request to the FastAPI model server to retrieve model metadata\n",
    "response = requests.get(FASTAPI_URL)\n",
    "\n",
    "# Handle the response from the server\n",
    "if response.status_code == 200:\n",
    "    model_info = response.json()  # Parse the JSON response containing the model details\n",
    "    print(\"Model Information:\")\n",
    "    print(\"Input Schema:\", model_info.get(\"input_schema\", \"No input schema information found\"))\n",
    "    print(\"Hyperparameters:\", model_info.get(\"hyperparameters\", \"No hyperparameters information found\"))\n",
    "    print(\"Important Features:\", model_info.get(\"important_features\", \"No important features information found\"))\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)  # Print any errors encountered during the request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights on Retrieved Model Information\n",
    "\n",
    "# Insert insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">F. Drift Detection Demonstration</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">Accessing Evidently AI from MLflow to Generate Drift Reports</h2>\n",
    "\n",
    "Once the `drift_reports` asset is activated within **Dagster UI**, these reports are subsequently recorded in **MLflow**, specifically under Experiments â†’ Artifacts.\n",
    "\n",
    "**How to Access Drift Reports in MLflow**\n",
    "\n",
    "1. Launch the **MLflow UI** by visiting http://localhost:5000.\n",
    "2. Proceed to **Experiments** and select the **Most Recent Run**.\n",
    "3. Navigate to the **Artifacts** tab.\n",
    "4. Find and download the `drift_report.html` to examine the comprehensive drift analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">Drift Result Analysis</h2>\n",
    "\n",
    "# Insert analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">F. Reproducability</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">G. References</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
