{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner.png](banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">A. System Overview</h2>\n",
    "\n",
    "This machine learning system is designed to showcase an end-to-end implementation covering the entire lifecycle of a machine learning model. It integrates various MLOps tools and technologies discussed during the DASCI 270 sessions, that includes facilitating data ingestion, preprocessing, training, validation, deployment, and monitoring of the model. The system is also structured to handle drift detection to ensure the model remains effective and accurate over time. This document will guide you through interacting with the deployed **Equipment Failure Prediction** system, including making predictions, retrieving model information, and checking data drift, as well as provide detailed documentation of each component and their functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">Components</h2>\n",
    "\n",
    "1. **Machine Learning Model** - This system utilizes XGBoost, a highly efficient and scalable machine learning algorithm for classification tasks. It serves as the core predictive model in our system, specifically designed to predict equipment failure.\n",
    "\n",
    "2. **Data Pipeline (Dagster)** - Orchestrates the workflow for data ingestion, preprocessing, and preparation. Dagster manages the sequence of these operations to ensure data flows correctly from one process to another, maintaining a clear and manageable execution order.\n",
    "\n",
    "3. **Experiment Tracking (MLflow)** - Provides a framework to track experiments, including model training runs, parameters, metrics, and artifacts, enabling easier debugging and optimization. It stores models, performance metrics, and custom objects like drift reports, making them easy to access and compare across different runs.\n",
    "\n",
    "4. **Model Serving (FastAPI)** - Deploys the trained model through a REST API using FastAPI, facilitating easy access to the model‚Äôs predictive capabilities. The API handles requests for predictions and provides model metadata, ensuring input validation and structured outputs.\n",
    "\n",
    "5. **Containerization (Docker)** - Containerizes the services making up the system to ensure consistency and reproducibility across environments. Each isolated environment (\"container\") contains all necessary dependencies for each service, which can be easily deployed on any system supporting Docker.\n",
    "\n",
    "6. **Drift Detection (Evidently AI)** - Integrates Evidently AI to monitor the model for any signs of data or concept drift. This component is crucial for maintaining the model's accuracy, providing insights into how the data characteristics and relationships are changing over time.\n",
    "\n",
    "7. **Data Validation (Pydantic)** - Ensures that the data received by the API matches the expected format and type. This prevents errors during model prediction and ensures reliable model performance.\n",
    "\n",
    "8. **Testing (Pytest and Github Actions)** - Uses Pytest to develop and run comprehensive tests that validate the correctness of the data processing and feature engineering components. GitHub Actions automates these tests, ensuring that all code integrations meet quality standards and function as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">Dataset</h2>\n",
    "\n",
    "This project utilized the AI4I 2020 Predictive Maintenance dataset, a synthetic dataset sourced from UCI Machine Learning Repository: https://archive.ics.uci.edu/dataset/601/ai4i+2020+predictive+maintenance+dataset. It contains 10,000 data points with the following columns/variables:\n",
    "\n",
    "| Variables          | Type        | Description                                                                                         |\n",
    "|--------------------|-------------|-----------------------------------------------------------------------------------------------------|\n",
    "| UID                | Integer     | Unique identifier ranging from 1 to 10000                                                           |\n",
    "| Product ID         | Categorical | Consists of a letter (L, M, H) for low, medium, and high product quality variants with a serial number |\n",
    "| Type               | Categorical | Not specified                                                                                       |\n",
    "| Air temperature    | Continuous  | Generated using a random walk process, normalized around 300 K with a standard deviation of 2 K      |\n",
    "| Process temperature| Continuous  | Generated using a random walk process, normalized to a standard deviation of 1 K, plus air temperature + 10 K |\n",
    "| Rotational speed   | Integer     | Calculated based on a power of 2860 W with normally distributed noise                               |\n",
    "| Torque             | Continuous  | Normally distributed around 40 Nm with a standard deviation of 10 Nm, no negative values             |\n",
    "| Tool wear          | Integer     | Tool wear added by quality variants H/M/L are 5/3/2 minutes respectively                             |\n",
    "| Machine failure    | Integer     | Indicates if the machine has failed (1) or not (0) in this particular datapoint                      |\n",
    "| TWF                | Integer     | Target feature indicating if a specific type of failure occurred or not (1/0)                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">B. Setup Instructions</h2>\n",
    "\n",
    "This section provides detailed instructions on how to set up and run the system using Docker Compose. The instructions assume you have Docker and Docker Compose installed on your machine. If not, please install them from the [Docker official website](https://www.docker.com/) before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">1. Clone the Repository</h2>\n",
    "\n",
    "Begin by cloning the project repository from GitHub. This will retrieve all the necessary code and configuration files needed to set up and run the project locally. Execute the following commands in your terminal:\n",
    "\n",
    "```bash\n",
    "# Clone the repository\n",
    "git clone https://github.com/raymundojavajr/ml-fp.git\n",
    "\n",
    "# Change directory to the cloned repository\n",
    "cd ml-fp\n",
    "```\n",
    "\n",
    "These commands clone the repository into a directory named `ml-fp` on your local machine and then change your current directory to `ml-fp`, placing you in the project's root directory where subsequent setup steps are performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">2. Configure Environment Variables</h2>\n",
    "\n",
    "To manage environment variables, create a `.env` file in the root directory of the project. This file will store all necessary environment settings. Ensure this file is not tracked by version control by adding it to your `.gitignore` file.\n",
    "\n",
    "For local development, you'll also need to set up a virtual environment to isolate your project's dependencies from the global Python environment. Here's how to set up and activate a virtual environment using Python's built-in `venv` module:\n",
    "\n",
    "```bash\n",
    "# Create a virtual environment\n",
    "python -m venv venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "# On Windows, use:\n",
    "venv\\Scripts\\activate\n",
    "\n",
    "# On Unix or MacOS, use:\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "If you are using uv as your package manager, ensure you have it installed and then synchronize your environment:\n",
    "\n",
    "```bash\n",
    "uv sync\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">3. Install Dependencies/Packages</h2>\n",
    "\n",
    "To ensure all functionalities of the pipeline work seamlessly, it is crucial to install the necessary Python packages. You can install dependencies using the `requirements.txt` for traditional pip installations or use `uv sync` which handles dependencies listed in `pyproject.toml`.\n",
    "\n",
    "**Using requirements.txt**\n",
    "```bash\n",
    "# Install dependencies from requirements.txt\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Using uv for pyproject.toml**\n",
    "If you are using `uv` as your package manager, you can synchronize your environment directly:\n",
    "\n",
    "```bash\n",
    "# Sync environment and install dependencies with uv\n",
    "uv sync\n",
    "```\n",
    "\n",
    "**Verify Installation**\n",
    "After installing the necessary packages, verify that all packages are correctly installed and configured to support all functionalities of the pipeline:\n",
    "\n",
    "```bash\n",
    "# List installed packages to verify installation\n",
    "pip list\n",
    "```\n",
    "\n",
    "This command displays all installed Python packages in your environment, allowing you to check that all required dependencies are correctly installed. This step is crucial for catching any missing or incorrectly installed packages before proceeding with further setup or development.\n",
    "\n",
    "#### **List of Libraries used in the pipeline:**\n",
    "\n",
    "The following libraries are integral to the pipeline, supporting various aspects of machine learning model development, deployment, and maintenance:\n",
    "\n",
    "**Core Libraries**\n",
    "\n",
    "- mlflow - Tracks experiments, including model training runs, parameters, and metrics.\n",
    "- pandas - Data manipulation and analysis.\n",
    "- matplotlib - Used for creating static, interactive, and animated visualizations in Python.\n",
    "- joblib - Efficient saving of models and other large data structures.\n",
    "- requests - Handles HTTP requests to external services or APIs.\n",
    "\n",
    "**Machine Learning Models and Tools**\n",
    "\n",
    "- xgboost - Implements machine learning algorithms under the Gradient Boosting framework.\n",
    "- scikit-learn - Tools for data mining and data analysis, including model training and evaluation functions.\n",
    "- shap - Explains the output of machine learning models.\n",
    "- evidently - Monitors model performance and detects data drift.\n",
    "\n",
    "**Dagster Integration**\n",
    "\n",
    "- dagster, dagit, dagster-postgres, dagster-pandas, dagster-mlflow - Provides orchestration of data workflows, asset management, and integration with other ML tools.\n",
    "\n",
    "**API Development and Deployment**\n",
    "\n",
    "- fastapi - Modern, fast (high-performance) web framework for building APIs.\n",
    "- uvicorn - ASGI server for FastAPI, which allows async programming.\n",
    "- pydantic - Data validation by defining data types and structures for FastAPI endpoints.\n",
    "- python-dotenv - Reads key-value pairs from a .env file and sets them as environment variables.\n",
    "\n",
    "**Utilities and Miscellaneous**\n",
    "\n",
    "- matplotlib - For plotting and visualizing data in various forms.\n",
    "- pydantic - For data validation and settings management via data models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">C. Orchestrating ML Workflows with Dagster</h2>\n",
    "\n",
    "This section explains the execution of the **Machine Failure Prediction** pipeline, which is fully managed within Dagster. This includes setting up and running the pipeline components via the Dagster UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline Overview**\n",
    "\n",
    "The pipeline is designed to handle various stages from data processing to model deployment, ensuring thorough analysis and prediction of machine failures.\n",
    "\n",
    "#### **Dagster Assets in the Pipeline**\n",
    "\n",
    "- **Data Ingestion (Upstream Assets):**\n",
    "  - `download_machine_data` retrieves raw sensor and operation data from sources.\n",
    "  - `raw_machine_data` stores the fetched data for subsequent processing.\n",
    "- **Data Preparation (Downstream from Raw Data):**\n",
    "  - `cleaned_machine_data` performs data cleaning and preprocessing to prepare for analysis.\n",
    "- **Model Development (Downstream from Prepared Data):**\n",
    "  - `train_test_split_data` segments the data into training and testing subsets.\n",
    "  - `feature_preprocessor` applies necessary transformations for the predictive model.\n",
    "  - `trained_model` encompasses the training of the failure prediction model.\n",
    "- **Operational Deployment (Downstream from Model Development):**\n",
    "  - `model_evaluation` assesses the predictive accuracy and robustness of the model.\n",
    "  - `saved_model` handles the storage of the validated model.\n",
    "- **Continuous Improvement (Downstream from Deployment):**\n",
    "  - `drift_detection_data` simulates potential drift in machine operation data.\n",
    "  - `cleaned_drift_data` prepares this data for drift analysis.\n",
    "  - `preprocessed_drift_data` further processes the data to fit analysis models.\n",
    "  - `drift_reports` creates detailed reports on any detected drift, helping in proactive maintenance.\n",
    "\n",
    "#### **Executing the Pipeline**\n",
    "\n",
    "To operationalize all assets in the **Machine Failure Prediction** pipeline, navigate to the **Dagster UI** at (`http://localhost:3000`) and follow these steps:\n",
    "\n",
    "- **Start with Upstream Assets:** Initiate the pipeline by materializing assets such as `download_machine_data` and `raw_machine_data`.\n",
    "- **Advance through the Pipeline:** Continue activating downstream assets in sequence to ensure a logical flow of data processing and model training.\n",
    "- **Finalize with Evaluation and Monitoring:** Complete the execution by materializing assets related to model evaluation and drift detection, which are crucial for maintaining the accuracy and relevance of the prediction model.\n",
    "\n",
    "This structured approach ensures that each component of the pipeline functions efficiently and that dependencies are correctly managed from data collection through to drift detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">D. Deploying the Full ML Pipeline Stack with Docker Compose</h2>\n",
    "\n",
    "This project leverages Docker Compose for containerization, ensuring cohesive and efficient functioning of all components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Containerized Services in `docker-compose.yaml`**\n",
    "\n",
    "Below are the services configured in the `docker-compose.yaml` file, outlining their roles and the ports they operate on:\n",
    "\n",
    "| **Service** | **Purpose** | **Port** |\n",
    "|-------------|-------------|---------|\n",
    "| **PostgreSQL (`postgres_service`)** | Database for storing pipeline data | Internal |\n",
    "| **MinIO (`minio_service`)** | S3-compatible storage for artifacts | `9000`, `9001` |\n",
    "| **MLflow Tracking Server (`mlflow_service`)** | Manages experiment tracking and artifact logging | `5000` |\n",
    "| **FastAPI Model Server (`fastapi_service`)** | Hosts the machine failure prediction model | `8000` |\n",
    "| **Dagster Server (`dagster_service`)** | Orchestrates and monitors the pipeline | `3000` |\n",
    "\n",
    "#### **Step 1: Launch All Services**\n",
    "Execute the following command to build and initiate all services:\n",
    "\n",
    "```bash\n",
    "docker-compose up --build -d\n",
    "```\n",
    "\n",
    "This command:\n",
    "- Constructs any necessary Docker images.\n",
    "- Launches all services in detached mode (`-d`).\n",
    "\n",
    "#### **Step 2: Verify Service Containers**\n",
    "To confirm that all Docker containers are active, run:\n",
    "\n",
    "```bash\n",
    "docker ps\n",
    "```\n",
    "\n",
    "#### **Step 3: Service Access Points**\n",
    "Here are the URLs to access each service's user interface:\n",
    "\n",
    "| **Service**               | **URL**                                 |\n",
    "|---------------------------|-----------------------------------------|\n",
    "| **Dagster UI**            | [http://localhost:3000](http://localhost:3000) |\n",
    "| **MLflow Tracking UI**    | [http://localhost:5000](http://localhost:5000) |\n",
    "| **FastAPI Model Server**  | [http://localhost:8000](http://localhost:8000) |\n",
    "| **MinIO Console**         | [http://localhost:9001](http://localhost:9001) |\n",
    "\n",
    "#### **Step 4: Configure MinIO for MLflow**\n",
    "Prior to using Dagster for asset materialization, configure a storage bucket in MinIO:\n",
    "\n",
    "1. Visit [http://localhost:9001](http://localhost:9001)\n",
    "2. Log in using:\n",
    "   - **Username:** `minio_user`\n",
    "   - **Password:** `minio_password`\n",
    "3. Establish a new bucket named `mlflow`.\n",
    "4. Amend your `.env` file with the MinIO Access Keys obtained:\n",
    "\n",
    "   ```ini\n",
    "   # MinIO Access Keys\n",
    "   MINIO_ACCESS_KEY=your_generated_access_key\n",
    "   MINIO_SECRET_ACCESS_KEY=your_generated_secret_key\n",
    "   ```\n",
    "\n",
    "#### **Step 5: Materialize Assets in Dagster**\n",
    "With all services operational, proceed to materialize assets:\n",
    "\n",
    "- Navigate to the [Dagster UI](http://localhost:3000).\n",
    "- Select \"Assets\" ‚Üí \"Materialize Selected\" to execute the pipeline.\n",
    "\n",
    "#### **Service Management Commands**\n",
    "\n",
    "| **Action**                | **Command**                          |\n",
    "|---------------------------|--------------------------------------|\n",
    "| **Restart all services**  | `docker-compose down && docker-compose up -d` |\n",
    "| **Stop all services**     | `docker-compose down`                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">E. Happy Path</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">Prediction Request</h2>\n",
    "\n",
    "In this section, we demonstrate how to use the FastAPI model server for predictions. We'll send a set of equipment parameters and status data to receive predictions on potential machine failures.\n",
    "\n",
    "- **Input Data:** This includes a series of variables such as equipment type, air and process temperatures, rotational speed, torque, tool wear, and previous instances of machine failure.\n",
    "- **API Endpoint:** This is the URL where the FastAPI server accepts prediction requests.\n",
    "- **Response Handling:** Upon receiving the prediction request, the server provides a forecast indicating whether a machine is likely to fail. This prediction is displayed in the output.\n",
    "\n",
    "Execute the following cell to initiate a prediction request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Prediction Response =====\n",
      "Status Code: 200\n",
      "Showing first 20 of 1000 predictions:\n",
      "  Row 1: 0\n",
      "  Row 2: 0\n",
      "  Row 3: 1\n",
      "  Row 4: 0\n",
      "  Row 5: 0\n",
      "  Row 6: 0\n",
      "  Row 7: 0\n",
      "  Row 8: 1\n",
      "  Row 9: 1\n",
      "  Row 10: 0\n",
      "  Row 11: 1\n",
      "  Row 12: 0\n",
      "  Row 13: 0\n",
      "  Row 14: 0\n",
      "  Row 15: 1\n",
      "  Row 16: 0\n",
      "  Row 17: 1\n",
      "  Row 18: 1\n",
      "  Row 19: 0\n",
      "  Row 20: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Load test data\n",
    "csv_path = \"synthetic_data.csv\"\n",
    "test_df = pd.read_csv(csv_path)\n",
    "json_data = test_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Define FastAPI Prediction Endpoint\n",
    "predict_url = \"http://localhost:8000/predict\"\n",
    "\n",
    "# Send Prediction Request\n",
    "predict_response = requests.post(predict_url, json=json_data)\n",
    "\n",
    "# Handle Response\n",
    "print(\"\\n===== Prediction Response =====\")\n",
    "print(\"Status Code:\", predict_response.status_code)\n",
    "\n",
    "try:\n",
    "    response_json = predict_response.json()\n",
    "    \n",
    "    # Extract predictions safely\n",
    "    predictions = response_json.get(\"predictions\", [])\n",
    "\n",
    "    if predictions:\n",
    "        print(f\"Showing first 20 of {len(predictions)} predictions:\")\n",
    "        for i, pred in enumerate(predictions[:20]):\n",
    "            print(f\"  Row {i+1}: {pred}\")\n",
    "    else:\n",
    "        print(\"No predictions returned.\")\n",
    "\n",
    "except requests.exceptions.JSONDecodeError:\n",
    "    print(\"Invalid response received from Prediction API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response Interpretation\n",
    "\n",
    "The response indicates that the request was successfully processed, as shown by the **Status Code: 200**. This confirms that the FastAPI service is running properly and returning predictions without any errors. The model processed **1,000 rows** from the test dataset, but only the first 20 predictions are displayed for reference.  \n",
    "\n",
    "Each prediction is represented as either **`0`** or **`1`**, where **`0`** likely indicates normal operation with no failure detected, while **`1`** suggests a potential failure that may require maintenance. For example, Row 1 has a prediction of **`0`**, meaning no failure was detected, while Row 3 has a prediction of **`1`**, indicating a possible failure. Similar patterns are observed in Rows 8, 11, and 15, where failure predictions appear sporadically.  \n",
    "\n",
    "The model does not predict failures in a uniform manner, suggesting that different conditions in the dataset contribute to varying predictions. This pattern aligns with real-world predictive maintenance scenarios, where failures occur intermittently rather than consistently across all cases.  \n",
    "\n",
    "### **Next Steps**  \n",
    "\n",
    "To gain deeper insights, it is important to analyze cases where failures (`1`) are predicted and identify common factors such as high tool wear, extreme temperatures, or unusual operational conditions. Additionally, running a **drift detection analysis** can help determine if the dataset has significantly changed over time, which could impact the model‚Äôs accuracy. Finally, evaluating the model‚Äôs **performance metrics** (such as accuracy, F1-score, and recall) will help ensure that the predictions remain reliable and actionable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">Model Information Retrieval</h2>\n",
    "\n",
    "To effectively interact with our FastAPI model server, it's essential to retrieve the model's configuration. This process involves accessing detailed metadata from the /model endpoint, which helps users understand the underlying model's structure and operational parameters.\n",
    "\n",
    "**Retrieve Model Information**\n",
    "\n",
    "The model server offers detailed metadata which includes:\n",
    "\n",
    "* **Input schema** detailing the required format for prediction inputs.\n",
    "* **Training Hyperparameters** used during the training process.\n",
    "* **Important features** that significantly impact the prediction outcomes.\n",
    "\n",
    "Execute the following cell to obtain detailed information about the model hosted on FastAPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model loaded successfully!\n",
      "üìå Model Type: <class 'xgboost.sklearn.XGBClassifier'>\n",
      "\n",
      "===== üîß Model Hyperparameters =====\n",
      "  objective: binary:logistic\n",
      "  base_score: None\n",
      "  booster: None\n",
      "  colsample_bylevel: None\n",
      "  colsample_bynode: None\n",
      "  colsample_bytree: None\n",
      "  device: None\n",
      "  eval_metric: logloss\n",
      "  gamma: None\n",
      "  grow_policy: None\n",
      "  interaction_constraints: None\n",
      "  learning_rate: None\n",
      "  max_bin: None\n",
      "  max_cat_threshold: None\n",
      "  max_cat_to_onehot: None\n",
      "  max_delta_step: None\n",
      "  max_depth: None\n",
      "  max_leaves: None\n",
      "  min_child_weight: None\n",
      "  monotone_constraints: None\n",
      "  multi_strategy: None\n",
      "  n_jobs: None\n",
      "  num_parallel_tree: None\n",
      "  random_state: 42\n",
      "  reg_alpha: None\n",
      "  reg_lambda: None\n",
      "  sampling_method: None\n",
      "  scale_pos_weight: None\n",
      "  subsample: None\n",
      "  tree_method: None\n",
      "  validate_parameters: None\n",
      "  verbosity: None\n",
      "\n",
      "===== üî• Feature Importances =====\n",
      "  UDI: 0.0086\n",
      "  Air_temperature_K: 0.0113\n",
      "  Process_temperature_K: 0.0085\n",
      "  Rotational_speed_rpm: 0.0105\n",
      "  Torque_Nm: 0.0080\n",
      "  Tool_wear_min: 0.0211\n",
      "  Type_encoded: 0.0000\n",
      "  Product_ID_encoded: 0.0069\n",
      "  Failure_Type_encoded: 0.9251\n",
      "\n",
      "‚úÖ Model trained on 9 features\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Define the model path\n",
    "model_path = \"C:/Users/karth/Documents/MSDS/ml-fp/src/models/trained_model.pkl\"\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load(model_path)\n",
    "print(\"\\nModel loaded successfully\")\n",
    "print(f\"Model Type: {type(model)}\")\n",
    "\n",
    "# Retrieve Model Hyperparameters\n",
    "params = model.get_xgb_params()\n",
    "print(\"\\n===== Model Hyperparameters =====\")\n",
    "for param, value in params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Retrieve Feature Importance\n",
    "if hasattr(model, \"feature_importances_\") and hasattr(model, \"feature_names_in_\"):\n",
    "    feature_importance = {feature: importance for feature, importance in zip(model.feature_names_in_, model.feature_importances_)}\n",
    "    print(\"\\n===== Feature Importances =====\")\n",
    "    for feature, importance in feature_importance.items():\n",
    "        print(f\"  {feature}: {importance:.4f}\")\n",
    "else:\n",
    "    print(\"\\nFeature importance is not available.\")\n",
    "\n",
    "# Verify Model Training\n",
    "if hasattr(model, \"n_features_in_\"):\n",
    "    print(f\"\\nModel trained on {model.n_features_in_} features\")\n",
    "else:\n",
    "    print(\"\\nModel might not be properly trained\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights on Retrieved Model Information\n",
    "\n",
    "### **Explanation of Your Model Output**\n",
    "\n",
    "####  **Model Type:** `<class 'xgboost.sklearn.XGBClassifier'>`\n",
    "- This means the model is an **XGBoost Classifier** (used for classification tasks).\n",
    "- It's designed to predict a categorical outcome (e.g., binary classification of failure detection in predictive maintenance).\n",
    "\n",
    "---\n",
    "\n",
    "## **Model Hyperparameters**\n",
    "These are the **settings** used when training the model.\n",
    "\n",
    "| **Hyperparameter**      | **Value**        | **Explanation** |\n",
    "|-------------------------|-----------------|-----------------|\n",
    "| **objective**          | `binary:logistic` | Model predicts probabilities for a binary classification problem (e.g., failure or no failure). |\n",
    "| **eval_metric**        | `logloss`        | The model was trained using **logarithmic loss**, a standard metric for classification. |\n",
    "| **random_state**       | `42`             | Ensures reproducibility of results by fixing the randomness. |\n",
    "| Other parameters       | `None`           | These hyperparameters were not explicitly set and used default values. |\n",
    "\n",
    " **Issue: Many hyperparameters are `None`**\n",
    "- This suggests that either they were not properly logged or the saved model did not retain all training details.\n",
    "- It might still work, but if you want a fully configured model, ensure **all necessary hyperparameters are explicitly set** when training.\n",
    "\n",
    "---\n",
    "\n",
    "## **Feature Importances**\n",
    "These indicate **how much each feature contributes to the model's decisions**.\n",
    "\n",
    "| **Feature**             | **Importance** | **Explanation** |\n",
    "|-------------------------|--------------|-----------------|\n",
    "| **UDI**                 | `0.0086`      | Has very little impact on predictions. |\n",
    "| **Air_temperature_K**   | `0.0113`      | Slightly more important, but still low. |\n",
    "| **Process_temperature_K** | `0.0085`    | Low impact. |\n",
    "| **Rotational_speed_rpm** | `0.0105`    | Low impact. |\n",
    "| **Torque_Nm**           | `0.0080`      | Low impact. |\n",
    "| **Tool_wear_min**       | `0.0211`      | More important than previous features. |\n",
    "| **Type_encoded**        | `0.0000`      | **Not contributing at all.** Might be irrelevant. |\n",
    "| **Product_ID_encoded**  | `0.0069`      | Very low importance. |\n",
    "| **Failure_Type_encoded** | **`0.9251`** | üö® **Dominates the model.** Almost all decisions are based on this feature. |\n",
    "\n",
    "üîç **Insight:** \n",
    "- The model **heavily relies on `Failure_Type_encoded` (92.51%)**, meaning it may not be using other features effectively.\n",
    "- If this feature is incorrectly encoded or biased, the model may **overfit** and generalize poorly.\n",
    "- Consider **retraining the model with proper feature engineering** to balance importance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model trained on 9 features**\n",
    "- The model recognizes **9 input features** from the dataset.\n",
    "- All of them are considered during training, but as seen in feature importance, some contribute **far more than others**.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">F. Drift Detection Demonstration</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To use the Drift Detection API, first, ensure that the Drift Monitoring Service is running by starting it with Docker using the command docker-compose up -d drift_monitoring_service. Next, prepare your input data in the correct format, ensuring it is structured as a list of dictionaries before making the request. Once the data is ready, execute the provided script in a Python environment such as Jupyter Notebook or a standalone script. After running the script, check the response. If the status code is 200, the request was successfully processed, and you can review the drift detection results. If drift is detected (True), it indicates significant changes in the input data compared to the reference dataset. If you encounter an invalid response, check the service logs by running docker logs drift_monitoring_service --tail 50 to diagnose potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Drift Detection Response =====\n",
      "Status Code: 200\n",
      "Response: {'drift_detected': True, 'drift_score': 0.0, 'drift_report_html_path': 'mlflow_artifacts/drift_report.html'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define Drift Detection Endpoint\n",
    "drift_url = \"http://localhost:8085/drift\"\n",
    "\n",
    "# Send Drift Detection Request\n",
    "drift_response = requests.post(drift_url, json=json_data)\n",
    "\n",
    "# Handle Response\n",
    "print(\"\\n===== Drift Detection Response =====\")\n",
    "print(\"Status Code:\", drift_response.status_code)\n",
    "\n",
    "try:\n",
    "    print(\"Response:\", drift_response.json())\n",
    "except requests.exceptions.JSONDecodeError:\n",
    "    print(\"‚ùå Drift Monitoring Service returned an invalid response.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    " The drift detection response indicates that drift has been detected in the dataset, as shown by \"drift_detected\": True. However, the drift score is 0.0, which suggests that while drift is flagged, no significant changes in feature distributions were quantified. This may be due to an issue with how the drift score is computed or an edge case in the data. The detailed drift analysis report has been saved as an HTML file at mlflow_artifacts/drift_report.html, which can be reviewed for a deeper understanding of which features have drifted and the extent of the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">Accessing Evidently AI from MLflow to Generate Drift Reports</h2>\n",
    "\n",
    "Once the `drift_reports` asset is activated within **Dagster UI**, these reports are subsequently recorded in **MLflow**, specifically under Experiments ‚Üí Artifacts.\n",
    "\n",
    "**How to Access Drift Reports in MLflow**\n",
    "\n",
    "1. Launch the **MLflow UI** by visiting http://localhost:5000.\n",
    "\n",
    "\n",
    "2. Proceed to **Experiments** and select the **Most Recent Run**.\n",
    "3. Navigate to the **Artifacts** tab.\n",
    "4. Find and download the `drift_report.html` to examine the comprehensive drift analysis. \n",
    "\n",
    "![Drift.png](Drift.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#d7633a; padding: 5px; text-align:left; border: 1px solid #d7633a;\">Drift Result Analysis</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drift detection report reveals that significant changes have occurred in the dataset, indicating that the distribution of new data differs considerably from the historical reference data. The analysis examined nine features, all of which exhibited drift, meaning that 100% of the dataset has changed in some way. Since the dataset drift flag is marked as `true`, this suggests that the model may not perform as expected on the new data, potentially leading to unreliable predictions.\n",
    "\n",
    "Several key features have undergone notable drift. For example, **air temperature** has a drift score of `0.6975`, indicating a significant shift in distribution. Similarly, **process temperature** (`0.7437`) and **rotational speed** (`0.5114`) show noticeable variations compared to the reference data. Additionally, **tool wear (`1.1567`) and failure type encoding (`1.4140`)** have experienced high levels of drift, suggesting that the nature of failures in the dataset may have changed over time. The **product ID encoding (`0.7212`) and torque (`0.2536`)** have also drifted, meaning that machine and operational characteristics may no longer match historical patterns.\n",
    "\n",
    "One of the most concerning drifts is seen in the **Unique Device Identifier (UDI) with a drift score of `1.5588`**, implying that the distribution of machines or components being monitored is significantly different from what the model was originally trained on. Additionally, the encoding of machine types has drifted (`0.2244`), indicating possible changes in the categories of equipment being processed.\n",
    "\n",
    "Given that all features have experienced drift, this suggests a fundamental shift in the dataset, which may impact the reliability of predictions. If this drift is due to changes in the operational environment, machine wear, or sensor readings, the model may need to be retrained with updated data to maintain accuracy. Further investigation is required to determine whether the drift is temporary or a sign of a long-term change in the system. Reviewing the **drift report in detail** and comparing it with model performance metrics will help decide if retraining is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">F. Reproducibility</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline is designed to be fully reproducible to facilitate reliable and consistent results in machine learning model development, testing, and deployment processes.\n",
    "\n",
    "#### **Steps to Ensure Reproducibility**\n",
    "\n",
    "1. **Running Services:** Make sure that FastAPI and MLflow services are active. These are essential for handling API requests and tracking experiments, respectively.\n",
    "\n",
    "    ```plaintext\n",
    "    FastAPI should be accessible at: http://localhost:8000\n",
    "    MLflow should be accessible at: http://localhost:5000\n",
    "    ```\n",
    "\n",
    "2. **Model Artifacts and Logs:** Confirm that the latest model artifacts and corresponding logs are properly generated and accessible in MLflow. This ensures that the models being deployed and tested are the latest iterations.\n",
    "\n",
    "3.  **Configuration and Environment Variables:** Check that all environment variables and configurations are set correctly. This includes database URIs, API keys, and other service-specific configurations that might affect the system's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#ffffff; background-color:#004aac; padding: 10px; text-align:left; border: 1px solid #004aac;\">G. References</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/601/ai4i+2020+predictive+maintenance+dataset)\n",
    "\n",
    "[2] [Dagster Documentation](https://docs.dagster.io/)\n",
    "\n",
    "[3] [MLflow Documentation](https://www.mlflow.org/docs/2.1.1/index.html?utm_source=google&utm_medium=cpc&utm_term=&utm_campaign=&gad_source=1&gclid=CjwKCAjwp8--BhBREiwAj7og1-SMU90CS2ghrTsi-kHfz67v2JAaQR5QQfRzhIKUK8VywBypvMULGxoCqhoQAvD_BwE)\n",
    "\n",
    "[4] [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
    "\n",
    "[5] [Docker Documentation](https://docs.docker.com/)\n",
    "\n",
    "[6] [Evidently AI Documentation](https://docs.evidentlyai.com/introduction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
